# New-York-Payroll-Analysis
Final Project Deliverable for Ironhack Data Analytics Bootcamp. Explores payroll data provided by the City of New York. 

## Project Description
The purpose of this project was to use NYC Payroll data to analyze which city agencies were reporting and accruing the most overtime, and to make suggestions about changes that can be made. Skills I demonstrate in this project include gathering data, cleaning it using Python, analyzing the data in Python and MySql (both individually as well as together, more on that later), and building visualizations using Tableau. The project also required the use of softer skills such as time-management, problem solving, work-prioritization, and gathering and presenting valuable insight. 

## Data
[NYC Citywide Payroll Data][1] - Because New York City is an open-data city, they publish data regarding city operations for anyone to access and take advantage of. This annually updata dataset lists all city employees, their starting dates, position titles, salaries, employing agencies, hours, and working locations since 2014. It also reports regular and overtime hours and pay. More can be read on it by accessing the link. 

[City and Town Population Totals: 2020-2023][2] - This U.S. Census data lists the populations of all cities and towns in the United States (with a population of more that 20,000) for 2020-2023. This dataset was only used in Tableau, as it was relevant to my visualization building, but not to my analysis. 

[1]: https://data.cityofnewyork.us/City-Government/Citywide-Payroll-Data-Fiscal-Year-/k397-673e/about_data        "NYC Citywide Payroll Data"
[2]: https://www.census.gov/data/tables/time-series/demo/popest/2020s-total-cities-and-towns.html                 "City and Town Population Totals: 2020-2023"

## Structure and Activities
### Data Cleaning
Data cleaning for this project was extensive and, aside from building my Tableau visualization, far and away occupied the greatest portion of my time. Some steps were easy, like the standardization of string columns contents to be all caps, the standardization of the dates columns using the pandas function to_datetime, and setting the column names to all be python and SQL friendly. Other cleaning activities included: 
* converting all **Work Location Borough** values that were not one of the main 5 boroughs to 'OTHER'.
* dropping name and initial columns after creation of **worker_id** column.
* reseting and creating a true index column
* creating columns providing more information about worker pay using mathematical operations from other columns
* adjusting column order to be more legible

I handled missing values as follows:
* **First Name**, **Last Name**, **Mid Init**: Some of these rows had no information in them because the City opted to withhold identifying information about certain individuals such as police, correction, and sanitation workers for their own protection. I deleted the Mid Init column, and filled missings in the other cells with "X"
* **Agency Start Date**, **Title Description** - Because there was no way to get accurate information for these rows, and because they needed to have something in them to be sent to a MySQL database, I just removed them. Given that there were only about 150 rows total where this was the case, it wasn't a significant loss. 
* **Work Location Borough**: This was the most troublesome column. I made two separate attempts to fill the columns more cleverly. First, I attempted to fill missings with the information from the cell above it if the location in the cells above and below were the same and if the employing agency in all three rows was the same. (The logic behind this being if this person is part of the same agency as the person listed before and after them, and the other two work in the same location, the person between them probably also works there.) However, when that only filled something like 50 rows, I took a more aggressive approach. I first added a new column that created a worker id for each and every individual by taking their initials, the initials of their employing agency, and their starting date (which would remain the same from year to year). (The remnants of this attempt, I am now realizing, are still visible in the data, because I kept the worker_id column for further analyzation that I never got to.) I then tried to fill the missings by having python look for the worker id to see if a working location showed up in any other of that employee's entries and use that. However, because that approach required sorting and then resorting my data back to it's original state, it ended up being far too memory-intensive and time-consuming to use. I ended up just filling all missing values in that column with 'WITHHELD'. Given that was the case for about 10% of my data, I didn't want to do that, but also given that the dataset still consisted of more than 4 million rows with accurate Working Location Borough data, I decided it was acceptable. 

After carrying out all of these activities, I combined them into a single function and copied that function into a python script file, so that the function could be called on in another notebook, thereby enabling me to easily separate my data cleaning from my data analysis. 

### Sending Data to a MySQL Database
It was in attempting to send my data that I thought was clean to a SQL database that I discovered the majority of the issues with my unclean data. Every problem with my data created an error, and stopped the transfer of information. After cleaning activity after cleaning activity in python, I employed the assistance of ChatGPT in setting up a process in which Python would send the dataset to SQL in chunks, document both in the notebook cell output as well as in a txt file the number of chunks that had been successfully uploaded, and resume an upload from the point that it left off on if it encountered an error and was interrupted. Of course, by the time I had gotten to that point, my data was cleaned sufficiently to upload in one attempt. But the code is still there, and I'm proud of it.

### Data Analysis
I conducted data analysis both in Python (using Visual Studio Code) and MySQL. Additionally, I put in extra effort to do some of the SQL analysis in Python using a SQL engine to input and execute queries. I used both SQL and pandas in python to save different subsets of the data as separate tables and dataframes. All my data findings pointed me towards things I wanted to visualize and explore more with Tableau. 

### Data Visualization
I built all my visualizations using Tableau, and it was in Tableau that I made use of the City Population data. (I also subsidized that data with a little bit of extra research to get the populations of each of the NYC boroughs). Rather than build a slides or Prezi presentation, I used my Tableau visualizations as my presentation, as I wanted to focus my presentation as much as possible on my findings rather than on what work I put into the project. 

## Findings and Conclusions
Data indicated that the five organizations that reported the most overtime hours per employee in 2023 were the Department of Correction, the Fire Department, the Housing Authority, the Police Department, and the Department of Sanitation. Each of these departments had their employees, on average, putting in enough hours of overtime to constitute 5 to 10 additional full working weeks per employee. The number of reported overtime hours for these organizations ranged between 2 million for the Department of Santitation and 13 million for the Police Department, meaning that each of these departments could hire and give full time schedules, pay, and benefits to between 1,194 people for the Department of Sanitation and 7,088 people for the Police Department. These needs aren't new - each of these organizations reported hiring far fewer new employees than their overtime reports indicate that they could make use of every year (except 2016) since 2014. 

## Recommendations
If I were making recommendations to policy-makers in New York City, I would recommend one of the following courses of action.

**Biggest "Bang for Buck"** - The Police Department far and away reported the most overtime hours: more than 13.6 million. Since those are overtime hours, they are paid at 150% the rate of normal hours, meaning that those 13 million hours of overtime actually cost the city as much as 20 million hours. One third of that money (the extra 50%) could be allotted toward increasing the pay and benefits of the Police Department, as well as towards improving training and accountability (for improving public image), and towards advertising Police Department jobs. Any combination of these would likely result in the Police Department recieving more applications and therefore more worker and officers. This would cut down significantly on the over-time being logged by that Department, and therefore make the biggest dent in the biggest reporter of overtime, and increasing the pay and benefits of the officers would also offset at least some of the income lost by those officers who potentially rely on overtime to provide for themselves and their families. 

**Alleviation of Greatest Suffering** - If the reported overtime hours are accurate, then workers in both the Department of Correction and the Fire Department are putting in enough overtime hours to draw in an extra 9.8 and 9.6 (respectively) full working weeks every year. In other words, by the time December comes around, they've put in as much worker as anyone else will do through the middle of next March. That much overtime, for their sake, is unacceptable, especially given that the work these people do often puts them either in danger or at risk of danger, and often doesn't allow them to be home with their families for holidays, and doesn't typically allow them to just work a regular 9 to 5 like the rest of us. Taking the same actions (increasing pay and benefits substantially, and conducting marketing campaigns) would go a long way towards populating these crucial departments with more staff, and would help to reduce the amount of extra work (because that's what overtime is) that workers for these departments have to do, and would make the greatest difference not necessarily for the city, but for the individual staff members of these crucial city departments.  
